'''
 python dynamic_run_bo.py --logfile log_0922.txt --workload MYSQL_YCSB_BB_FILTERED --alpha 0.3 --beta 0.3 --gamma 0.3 --iter 500
'''

import sys, os
import torch
import re
import pandas as pd
import numpy as np
import ast
import joblib
from typing import Dict, Any, Optional, Tuple, List, Set
import tempfile
import logging
import argparse
from dotenv import load_dotenv  # pip install python-dotenv
import openai
from tabulate import tabulate # For table logging
import csv
from datetime import datetime
sys.path.append(os.path.abspath("..")) # í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ ê¸°ì¤€ ìƒìœ„ í´ë” ì¶”ê°€
from knob_selection.dynamic_knob_selection import knob_selection_with_lasso

# BoTorch
from botorch.models.model import Model
from botorch.models import SingleTaskGP
from botorch.fit import fit_gpytorch_mll
from botorch.acquisition import UpperConfidenceBound, AcquisitionFunction, ExpectedImprovement
from botorch.optim import optimize_acqf
from gpytorch.mlls import ExactMarginalLogLikelihood
from botorch.exceptions import BadInitialCandidatesWarning
import warnings

# Scikit-learn
from sklearn.preprocessing import MinMaxScaler

# ì‚¬ìš©ì ì •ì˜ ëª¨ë“ˆ
from knob_dependency_score import DependencyScore
from LLM_expert import query_openai, parse_llm_response, build_dependency_prompt


load_dotenv()
api_key = os.getenv("OPENAI_API_KEY")
assistant_id = os.getenv("OPENAI_ASSISTANT_ID")
# ì¿¼ë¦¬ í˜¸ì¶œ í•¨ìˆ˜
openai.api_key = api_key


# --- ëª…ë ¹ì¤„ì¸ì ---
parser = argparse.ArgumentParser()

# logfile ì´ë¦„
parser.add_argument("--logfile", type=str, required=True)
# worklaod ì¢…ë¥˜ ì´ë¦„
parser.add_argument("--workload", type=str, required=True)
# dependency score parameter(ë¯¼ê°ë„ íŒŒë¼ë¯¸í„°, ê°’ì´ ì‘ì„ìˆ˜ë¡ ì™„ë§Œí•´ì§€ëŠ” ê·¸ë˜í”„, ë” ë¯¼ê°í•œ ì°¨ì´ë„ ì˜ ë°˜ì˜í•¨)
parser.add_argument("--alpha", type=float, required=True)
parser.add_argument("--beta", type=float, required=True)
parser.add_argument("--gamma", type=float, required=True)
# bayesian optimization iteration íšŸìˆ˜
parser.add_argument("--iter", type=int, required=True)

args = parser.parse_args()

# --- ë¡œê¹… ì„¤ì • ---
log_formatter = logging.Formatter('%(asctime)s [%(levelname)s] %(message)s')
logger = logging.getLogger('BO_Logger')
logger.setLevel(logging.INFO) # ë¡œê·¸ ë ˆë²¨ ì„¤ì •

# íŒŒì¼ í•¸ë“¤ëŸ¬ ì„¤ì • (ì‹¤ì‹œê°„ ê¸°ë¡)
log_file_path = f'../../../data/bo_result/{args.logfile}.log'
file_handler = logging.FileHandler(log_file_path, mode='a', encoding = 'utf-8') # 'a' ëª¨ë“œë¡œ ì´ì–´ì“°ê¸°
file_handler.setFormatter(log_formatter)
# íŒŒì¼ í•¸ë“¤ëŸ¬ ì¶”ê°€ ì‹œ ì¦‰ì‹œ íŒŒì¼ì— ì“°ë„ë¡ ì„¤ì • (ë²„í¼ë§ ìµœì†Œí™”)
# Python 3.7+ ì—ì„œëŠ” FileHandlerê°€ ê¸°ë³¸ì ìœ¼ë¡œ ë²„í¼ë§í•˜ì§€ ì•ŠìŒ, ëª…ì‹œì  flush ë¶ˆí•„ìš”
logger.addHandler(file_handler)

warnings.filterwarnings("ignore", category=BadInitialCandidatesWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning)

# --- 3. Log Parsed Arguments as a Table (ìˆ˜ì •ëœ ë°©ì‹) ---
logger.info("") # ë¡œê·¸ ì‹œì‘ ì „ ë¹ˆ ì¤„ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)
logger.info("=" * 60)
logger.info("ğŸš€ Script Execution Started with Arguments:")
logger.info("=" * 60)

# Convert args namespace to dictionary
args_dict = vars(args)

# Create table string using tabulate
args_table = tabulate(args_dict.items(), headers=["Argument", "Value"], tablefmt="grid")

# <<< í•µì‹¬ ìˆ˜ì •: í…Œì´ë¸”ì„ í•œ ì¤„ì”© ë¡œê¹… >>>
for line in args_table.splitlines():
    logger.info(line)
# <<< ìˆ˜ì • ë >>>

logger.info("-" * 60)
logger.info("") # ë¡œê·¸ ëë‚œ í›„ ë¹ˆ ì¤„ ì¶”ê°€ (ì„ íƒ ì‚¬í•­)

# --- ì´ˆê¸° ì„¤ì • ë° ê²½ë¡œ ---
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
DTYPE = torch.double

# --- BO ì„¤ì • ---
PERFORMANCE_THRESHOLD = 1.1 # ì„±ëŠ¥ 10í”„ë¡œ ì´ìƒ í–¥ìƒ(1.1~1.3)
INITIAL_DEPENDENCY_WEIGHT = 1.0
RANDOM_STATE = 42
torch.manual_seed(RANDOM_STATE)

# --- ëª©ì  í•¨ìˆ˜ ê´€ë ¨ ì„¤ì • ---
TPS_METRIC_INDEX = 0 # 0: tps, 1: latency
LTC_METRIC_INDEX = 1
IS_MAXIMIZATION = True
N_ITERATIONS = args.iter

# --- Dynamic Knob Selection ê´€ë ¨ ìƒìˆ˜ ---
SELECTION_PERIOD = 5
TOP_K = 10
REPLACEMENT_RATIO = 0.2
TARGET_FOR_SELECTION = "tps/latency"
KNOB_INFO_CSV = "Knob_Information_MySQL_v5.7.csv"
SELECTION_DUMP_PATH = os.path.join("../../../data/bo_result", "prev_knob_selection.txt")
EXTRA_ACTIVATE_MAX = 10 # LLMì´ ì¶”ì²œí•œ inactive ì¤‘ 'ì¶”ê°€ í™œì„±' knob ìµœëŒ€ ê°œìˆ˜

# --- ë™ì  selection ìƒíƒœ ì „ì—­
active_knob_indices: Set[int] = set() # í˜„ì¬ í™œì„±ì‹œì¼œ íƒìƒ‰í•œ feature ì¸ë±ìŠ¤

# ìµœê·¼ ë¹„í™œì„±í™” ëœ knob ì´ë ¥ì„ (index, name, timestamp)ë¡œ ëˆ„ì  (part 2)
deactivated_history: List[Tuple[int, str, datetime]] = []

# ëˆ„ì  ë¹„í™œì„± ì§‘ê³„ ìœ í‹¸(ì „ì—­/ìœ í‹¸ ê·¼ì²˜)
def _cumulative_inactive_names(limit: int=1000) -> List[str]:
    names, seen = [], set()
    for _, name, _ in reversed(deactivated_history):
        if name not in seen:
            names.append(name); seen.add(name)
        if len(names) >= limit:
            break
    return names

# LLM í˜¸ì¶œ í—¬í¼(í™•ì¥ ì „ìš©) ì¶”ê°€
def _llm_select_correlated_from_sets(
    active_names: List[str],
    inactive_names: List[str],
    logger: logging.Logger,
    max_return: int=10,
) -> List[str]:
    try:
        # (w/ LLM) active_knobs + inative_knobsë¥¼ inputìœ¼ë¡œ ì£¼ê³ , correlation ì¶”ë¡ 
        prompt = (
            "# Given Knobs\n"
            "ActiveKnobs:\n" + ", ".join(active_names) + "\n\n"
            "InactiveKnobs (previously-active):\n" + ", ".join(inactive_names) + "\n\n"
            "# Task\n"
            "From ONLY the knob names above, identify knobs that have clear correlation "
            "relationships with the ACTIVE set (functional or operational coupling). "
            f"Return at most {max_return} names strictly from the provided lists, in JSON-like form:\n"
            "knob_names=[name1, name2, ...]\n"
            "If there are no such dependencies, return an empty list exactly as: knob_names=[]\n"
            "Do not invent new names."
        )

        llm_resp = query_openai(prompt)
        parsed = parse_llm_response(llm_resp) or {}
        result = parsed.get("knob_names", []) or []

        # filter: ë°˜ë“œì‹œ inputì— í¬í•¨ëœ knob nameë§Œ í—ˆìš©í•˜ë„ë¡ í•„í„°ë§
        provided = set(active_names) | set(inactive_names)
        filtered = [n for n in result if n in provided]

        logger.info("ğŸ¤– LLM correlated (names-only): %s", filtered[:10])
        return filtered[:max_return]
    except Exception as e:
        logger.info(f"WARN: LLM correlation selection failed (names-only): {e}")
        return []
        





def _recent_inactive_names(limit: int=20) -> List[str]:
    # ê°€ì¥ ìµœê·¼ì— ë¹„í™œì„±í™”ëœ knob ì´ë¦„ë“¤(ì¤‘ë³µ ì œê±°, ìµœì‹  ìš°ì„ )
    names=[]
    seen=set()
    for idx, name, ts in reversed(deactivated_history):
        if name not in seen:
            names.append(name)
            seen.add(name)
        if len(names) >= limit:
            break
    return names


# ex) fixed_features_current = {0: 0.50, 1: 0.17, 3: 0.81}
# ì˜ë¯¸: 0,1,3ë²ˆ ì°¨ì›ì€ ê°ê° 0.50, 0.17, 0.81ë¡œ 'ê³ ì •'í•´ì„œ ìµœì í™”í•œë‹¤.
fixed_features_current: Optional[Dict[int, float]] = None # ë¹„í™œì„± featureë¥¼ ê³ ì •í•  ê°’
'''
[Dynamic Knob Selection] êµ¬í˜„ë¶€
'''

def _safe_read_selected_knobs_from_csv(path: str) -> List[str]:
    # knob_selection_with_lassoê°€ ì €ì¥í•œ out_csvì—ì„œ knob ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ì–´ì˜¨ë‹¤.
    try:
        df = pd.read_csv(path)
        if "knob_name" in df.columns:
            return df["knob_name"].dropna().astype(str).tolist()
        cols = [c for c in df.columns if "knob" in c.lower()]
        if cols:
            print(f"[knob selection outputì„ ì½ì–´ì˜¨ë‹¤.]\n{cols}")
            return df[cols[0]].dropna().astype(str).tolist()
    except Exception as e:
        logger.info(f"WARN: selected knobs csv read fail: {e}")
    return []

def _indices_of(names: List[str], universe: List[str]) -> List[int]:
    # ì´ë¦„ ë¦¬ìŠ¤íŠ¸ -> ì „ì²´ì—ì„œì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    idx = []
    pos = {name: i for i, name in enumerate(universe)}
    for n in names:
        if n in pos:
            idx.append(pos[n])
    return idx

def _merge_active_set_with_limit(
        prev_active: Set[int],
        ranked_new: List[int],
        dim: int,
        replacement_ratio: float,
        top_k: int
    ) -> Set[int]:
        """
        prev_activeì—ì„œ ë„ˆë¬´ í° ë³€í˜•ì„ í”¼í•˜ê¸° ìœ„í•´,
        ìƒˆ ë­í‚¹(ranked_new) ê¸°ì¤€ìœ¼ë¡œ 'ìµœëŒ€ ceil(top_k * replacement_ratio)'ê¹Œì§€ë§Œ êµì²´.
        1) prev_active âˆ© ranked_new(ìƒìœ„ top_k) ìœ ì§€
        2) ë‚¨ëŠ” ìë¦¬ëŠ” ranked_new ìˆœì„œëŒ€ë¡œ êµì²´ (ìµœëŒ€ max_swap)
        3) ê·¸ë˜ë„ ëª¨ìë¼ë©´ prev_activeì—ì„œ carry-over
        4) ê·¸ë˜ë„ ëª¨ìë¼ë©´ 0..dim-1ì—ì„œ filler
        """
        import math

        max_swap = max(1, math.ceil(top_k * replacement_ratio))
        new_target = ranked_new[:top_k]  # ìƒìœ„ top_kë§Œ ê³ ë ¤

        # 1) ìœ ì§€ ë° í›„ë³´
        keep = [i for i in new_target if i in prev_active]
        candidates = [i for i in new_target if i not in prev_active]

        # 2) êµì²´ ìˆ˜ ì œí•œ ì ìš©
        add = candidates[:max_swap]

        # ë¶€ë¶„ ê²°ê³¼
        result = list(keep) + add

        # 3) ë¶€ì¡±ë¶„ì€ ì´ì „ activeì—ì„œ carry-over
        if len(result) < top_k:
            carry = [i for i in prev_active if i not in result]
            carry = carry[:(top_k - len(result))]
            result += carry
        else:
            carry = []

        # 4) ê·¸ë˜ë„ ëª¨ìë¼ë©´ filler
        if len(result) < top_k:
            filler = [i for i in range(dim) if i not in result]
            filler = filler[:(top_k - len(result))]
            result += filler
        else:
            filler = []

        # ìµœì¢… ì§‘í•©(ì •í™•íˆ top_kê°œë¡œ ìë¥´ê¸°)
        final_set = set(result[:top_k])

        # ğŸ  ë””ë²„ê·¸ ë¡œê·¸ (í•­ìƒ ì°í˜)
        print(f"ğŸ  keep        : {keep}")
        print(f"ğŸ  add(swapped): {add} (max_swap={max_swap})")
        print(f"ğŸ  carry-over  : {carry}")
        print(f"ğŸ  filler      : {filler}")
        print(f"ğŸ  final_set   : {sorted(final_set)} (size={len(final_set)})")

        return final_set


# ë¬¸ì„œ ê¸°ë³¸ê°’ì„ [0, 1]ë¡œ ìŠ¤ì¼€ì¼ í•´ ì–»ê¸°
# input: ì „ì²´ knob ì´ë¦„ ìˆœì„œ, ì´ë¯¸ fitëœ scaler, info ê²½ë¡œ
def load_mysql_defaults_scaled(knob_names: list, x_scaler: MinMaxScaler, info_csv="Knob_Information_MySQL_v5.7.csv")-> np.ndarray:
    info_df = pd.read_csv(info_csv)
    info_df.columns = info_df.columns.str.strip().str.replace("\ufeff", "", regex=False)
    raw = dict(zip(info_df["name"], info_df["d_f_default"])) # scale ì´ì „ value

    def to_float(v):
        s = str(v).strip().lower()
        if s in ("on", "true", "yes"): return 1.0
        if s in ("off", "false", "no"): return 0.0
        return float(s)

    # 1. info_csvì—ì„œ ê° knobì˜ ë¬¸ì„œ ê¸°ë³¸ê°’ì„ ì½ìŒ
    # 2. knob_names ìˆœì„œëŒ€ë¡œ ë²¡í„°í™”
    # 3. x_scaler.transform(...)ìœ¼ë¡œ [0, 1] ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    defaults = np.array([to_float(raw[k]) for k in knob_names], dtype=float) # (DIM, )
    defaults_scaled = x_scaler.transform([defaults]).astype(np.float64) # (1, DIM)
    return np.clip(defaults_scaled, 0.0, 1.0)

def _build_fixed_features_for_inactive(dim: int, active: Set[int], best_x_scaled_tensor: Optional[torch.Tensor]) -> Dict[int, float]:
    """
    ë¹„í™œì„± featureëŠ” í˜„ì¬ best ì„¤ì •(ìŠ¤ì¼€ì¼ ê³µê°„) ê°’ìœ¼ë¡œ ê³ ì •.
    bestê°€ ì•„ì§ ì—†ë‹¤ë©´ 0.5ë¡œ ê³ ì •.
    # example)  
    best_x_scaled_tensor = array([0.80, 0.35, 0.62, 0.50, 0.10]) / shape = (1, DIM)
    best, best[j] = ê° ì¸ë±ìŠ¤ jì— ëŒ€í•œ ìµœê³  êµ¬ì„±ì˜ jë²ˆì§¸ knobì˜ ìŠ¤ì¼€ì¼ ê°’(range: [0, 1])
    """
    fixed = {}
    if best_x_scaled_tensor is not None:
        best = best_x_scaled_tensor.view(-1).detach().cpu().numpy()
    else:
        best = None
    for j in range(dim):
        if j not in active:
            # fixed ë°°ì—´ì— ê³ ì • feature value ì €ì¥
            fixed[j] = float(best[j]) if best is not None else 0.5
    return fixed

# í•´ë‹¹ ì‹œì ì˜ í™œì„± knob ì§‘í•©(active set)ì„ íŒŒì¼ë¡œ ì €ì¥í•´ë‘ëŠ” ë¡œê·¸/ì¬í˜„ìš© ìŠ¤ëƒ…ìƒ·
def _dump_selection(knob_names: List[str], active: Set[int], path: str):
    try:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write("# active knob subset (indices then names)\n")
            act = sorted(list(active))
            f.write(",".join(map(str, act)) + "\n")
            f.write(",".join([knob_names[i] for i in act]) + "\n")
        logger.info(f"INFO: saved active subset -> {path}")
    except Exception as e:
        logger.info(f"WARN: selection dump failed: {e}")

# (ì½”ë“œ ì´ë™)

'''
# í•¨ìˆ˜ ì •ì˜
    ï¿® load_data_and_components: df, xgb_model, x_scaler, y_scaler, knob_names ë¶ˆëŸ¬ì˜¤ëŠ” í•¨ìˆ˜
    ï¿® prepare_initial_data:  X(conf), Y(perf) ë°ì´í„° MinMaxScalingí•˜ëŠ” í•¨ìˆ˜
    ï¿® predict_performance_scaled: performance ì˜ˆì¸¡ê°’ ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜(í•™ìŠµëœ xgboost ë¡œë“œ)
    ï¿® calculate_dependency_weight: knobê°„ì˜ dependency_score ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜
    ï¿® parse_llm_response: LLM output íŒŒì‹±í•˜ëŠ” í•¨ìˆ˜(dependency_score í•¨ìˆ˜ì— ë„£ì„ ë§¤ê°œë³€ìˆ˜ íŒŒì‹±)
    ï¿® get_fitted_model: GP(surrogate model) ì—…ë°ì´íŠ¸ í•¨ìˆ˜
    ï¿® find_best_initial_point_scaled: ì´ˆê¸° ë°ì´í„°ì…‹ì¤‘ ê°€ì¥ ì„±ëŠ¥ ì¢‹ì€ ë°ì´í„° ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜(íƒìƒ‰ ì‹œì‘ì  ì„¤ì •)
    ï¿® select_random_initial_point_scaled: ì´ˆê¸° ë°ì´í„°ì…‹ì¤‘ ëœë¤ ë°ì´í„° ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜(íƒìƒ‰ ì‹œì‘ì  ì„¤ì •)
    ï¿® select_random_initial_point_scaled: ì´ˆê¸° ë°ì´í„°ì…‹ì¤‘ ëœë¤ ë°ì´í„° ë°˜í™˜í•˜ëŠ” í•¨ìˆ˜(íƒìƒ‰ ì‹œì‘ì  ì„¤ì •)
    â—† DependencyWeightedAcquisitionFunction: ì»¤ìŠ¤í…€ Acquisition í•¨ìˆ˜ ì •ì˜ í´ë˜ìŠ¤
        ï¿®
        ï¿®
'''
global tps_importance
tps_importance = 2.0

# (ì°¸ì¡°) ì´ê²ƒë„ knob selection ì£¼ê¸°ë§ˆë‹¤ ì¬í•™ìŠµ í•´ì•¼í•˜ë‚˜???
def load_data_and_components(csv_path: str, model_path: str, x_scaler_path: str, y_scaler_path: str, knob_names_path: str) \
        -> Tuple[pd.DataFrame, Any, MinMaxScaler, MinMaxScaler, list]:
    """ë°ì´í„°, ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, Knob ì´ë¦„ì„ ë¡œë“œ"""
    # ì´ì „ ë‹µë³€ê³¼ ë™ì¼
    logger.info("--- ë°ì´í„° ë° êµ¬ì„± ìš”ì†Œ ë¡œë”© ì‹œì‘ ---")
    try:
        df = pd.read_csv(csv_path)
        logger.info(f"âœ… CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {csv_path} (Shape: {df.shape})")
        xgb_model = joblib.load(model_path)
        logger.info(f"âœ… XGBoost ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
        x_scaler = joblib.load(x_scaler_path)
        logger.info(f"âœ… X Scaler ë¡œë“œ ì™„ë£Œ: {x_scaler_path}")
        y_scaler = joblib.load(y_scaler_path)
        logger.info(f"âœ… Y Scaler ë¡œë“œ ì™„ë£Œ: {y_scaler_path}")
        knob_names = joblib.load(knob_names_path)
        logger.info(f"âœ… Knob ì´ë¦„ ë¡œë“œ ì™„ë£Œ: {knob_names_path} ({len(knob_names)}ê°œ)")
        logger.info("--- ëª¨ë“  êµ¬ì„± ìš”ì†Œ ë¡œë”© ì„±ê³µ ---\n")
        return df, xgb_model, x_scaler, y_scaler, knob_names
    except FileNotFoundError as e: print(f"âŒ ì˜¤ë¥˜: í•„ìˆ˜ íŒŒì¼ ì—†ìŒ - {e}"); exit()
    except Exception as e: print(f"âŒ ì˜¤ë¥˜: íŒŒì¼ ë¡œë”© ì¤‘ ë¬¸ì œ - {e}"); exit()

def prepare_initial_data(df: pd.DataFrame, knob_names: list, x_scaler: MinMaxScaler, y_scaler: MinMaxScaler) \
        -> Tuple[torch.Tensor, torch.Tensor, np.ndarray, np.ndarray]:
    """ë¡œë“œëœ Scalerë¡œ ì´ˆê¸° ë°ì´í„°ë¥¼ ìŠ¤ì¼€ì¼ë§í•˜ê³  í…ì„œ/ì›ë³¸ ë°°ì—´ ë°˜í™˜."""
    # ì´ì „ ë‹µë³€ê³¼ ë™ì¼
    logger.info("--- ì´ˆê¸° ë°ì´í„° ì¤€ë¹„ ì‹œì‘ ---")
    try:
        X_all = df[knob_names].values
        Y_all = df[['tps', 'latency']].values
        logger.info(f"ì›ë³¸ ë°ì´í„° Shape: X={X_all.shape}, Y={Y_all.shape}")
        scaled_X_all = x_scaler.transform(X_all)
        scaled_Y_all = y_scaler.transform(Y_all)
        logger.info("âœ… ë°ì´í„° ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ ([0, 1] ë²”ìœ„)")
        logger.info(f"âœ… scaled_X_all: {scaled_X_all}")
        logger.info(f"âœ… scaled_Y_all: {scaled_Y_all}")

        epsilon = 1e-9
        scaled_scores = (tps_importance*scaled_Y_all[:, 0])/(scaled_Y_all[:, 1]+epsilon)  #shape: (N, )
        train_X = torch.tensor(scaled_X_all, device=DEVICE, dtype=DTYPE)
        train_Y = torch.tensor(scaled_scores, device=DEVICE, dtype=DTYPE).unsqueeze(-1)
        logger.info(f"ì´ˆê¸° í•™ìŠµ í…ì„œ ìƒì„± ì™„ë£Œ: train_X={train_X.shape}, train_Y={train_Y.shape}")
        logger.info("--- ì´ˆê¸° ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ ---\n")
        return train_X, train_Y, X_all, Y_all # ì›ë³¸ ë°°ì—´ë„ ë°˜í™˜
    except KeyError as e: logger.info(f"âŒ ì˜¤ë¥˜: CSV ì»¬ëŸ¼ ì—†ìŒ - {e}"); exit()
    except Exception as e: logger.info(f"âŒ ì˜¤ë¥˜: ì´ˆê¸° ë°ì´í„° ì¤€ë¹„ ì¤‘ ë¬¸ì œ - {e}"); exit()

# --- XGBoost ì˜ˆì¸¡ í•¨ìˆ˜ (ìŠ¤ì¼€ì¼ë§ëœ ì…ë ¥/ì¶œë ¥ ë²„ì „) ---
def predict_performance_scaled(scaled_input_np: np.ndarray, xgb_model: Any) \
        -> Optional[Dict[str, float]]:
    """
    ìŠ¤ì¼€ì¼ë§ëœ knob ì„¤ì •(scaled_input_np)ì„ ë°›ì•„ XGBoost ëª¨ë¸ë¡œ
    ìŠ¤ì¼€ì¼ë§ëœ ì„±ëŠ¥ {'scaled_tps': ..., 'scaled_latency': ...} ë”•ì…”ë„ˆë¦¬ ë°˜í™˜
    """
    # ì…ë ¥ shape í™•ì¸ (1, DIM)
    if scaled_input_np.shape != (1, len(knob_names)):
         logger.info(f"âŒ ì˜¤ë¥˜: predict_performance_scaled ì…ë ¥ shape ì˜¤ë¥˜ - ê¸°ëŒ€: (1, {len(knob_names)}), ì‹¤ì œ: {scaled_input_np.shape}")
         return None

    # logger.info(f"--- ìŠ¤ì¼€ì¼ë§ëœ ì„±ëŠ¥ ì˜ˆì¸¡ ì‹œì‘ ---")
    try:
        # XGBoost ëª¨ë¸ ì˜ˆì¸¡ (ìŠ¤ì¼€ì¼ë§ëœ X ì…ë ¥ -> ìŠ¤ì¼€ì¼ë§ëœ Y ì¶œë ¥ ê°€ì •)
        scaled_predictions = xgb_model.predict(scaled_input_np) # shape (1, 2) ê°€ì •

        scaled_tps = scaled_predictions[0, 0]
        scaled_latency = scaled_predictions[0, 1]

        return {'scaled_tps': scaled_tps, 'scaled_latency': scaled_latency}

    except Exception as e:
        logger.info(f"âŒ ì˜¤ë¥˜: XGBoost ì˜ˆì¸¡ ì¤‘ ë¬¸ì œ ë°œìƒ - {e}")
        return None

# --- ì˜ì¡´ì„± ê°€ì¤‘ì¹˜ ê³„ì‚° í•¨ìˆ˜ ---
# ì…ë ¥: ì›ë³¸ ìŠ¤ì¼€ì¼ ê°’ë“¤, ë°˜í™˜: ê°€ì¤‘ì¹˜ (float)
def calculate_dependency_weight(
    prev_config_scaled: Optional[Dict[str, float]],
    prev_perf_scaled: Optional[Dict[str, float]],
    curr_config_scaled: Dict[str, float],
    curr_perf_scaled: Dict[str, float],
    knob_names: list
) -> float:

    weight = INITIAL_DEPENDENCY_WEIGHT
    if prev_perf_scaled is None or prev_config_scaled is None:
        logger.info("INFO: ì²« ë°˜ë³µ, ê¸°ë³¸ ê°€ì¤‘ì¹˜ 1.0 ì‚¬ìš©.")
        return weight

    # ì„±ëŠ¥ ë³€í™” ê³„ì‚° (ìŠ¤ì¼€ì¼ë§ëœ ê°’ ê¸°ì¤€)
    # í‚¤ ì´ë¦„ì´ 'scaled_tps', 'scaled_latency' ì„ì— ì£¼ì˜
    prev_latency_scaled = prev_perf_scaled.get('scaled_latency', 0)
    curr_latency_scaled = curr_perf_scaled.get('scaled_latency', 0)
    prev_tps_scaled = prev_perf_scaled.get('scaled_tps', 0)
    curr_tps_scaled = curr_perf_scaled.get('scaled_tps', 0)

    # ìŠ¤ì¼€ì¼ë§ëœ ê°’ìœ¼ë¡œ ì„±ëŠ¥ ë¹„ìœ¨ ê³„ì‚° (ì´ ë¹„êµ ë°©ì‹ì´ ìœ íš¨í•œì§€ í™•ì¸ í•„ìš”)
    # ì˜ˆë¥¼ ë“¤ì–´, latencyê°€ 0ì— ê°€ê¹Œì›Œì§€ë©´ ë¹„ìœ¨ì´ ë¶ˆì•ˆì •í•´ì§ˆ ìˆ˜ ìˆìŒ
    prev_metric = prev_tps_scaled / (prev_latency_scaled + 1e-9)
    curr_metric = curr_tps_scaled / (curr_latency_scaled + 1e-9)

    # ìŠ¤ì¼€ì¼ë§ëœ ê°’ ê¸°ì¤€ì˜ ì„±ëŠ¥ í–¥ìƒ ì„ê³„ê°’ ë¹„êµ
    perf_improvement = curr_metric / prev_metric
    logger.info(f"ğŸ“ˆ ì„±ëŠ¥ í–¥ìƒê°’: {perf_improvement}")
    if perf_improvement >= PERFORMANCE_THRESHOLD: # PERFORMANCE_THRESHOLD ê°’ì˜ ì˜ë¯¸ê°€ ë‹¬ë¼ì§
        logger.info("ğŸ”¥ INFO: ìœ ì˜ë¯¸í•œ ì„±ëŠ¥ í–¥ìƒ ê°ì§€ (ìŠ¤ì¼€ì¼ë§ ê°’ ê¸°ì¤€), LLM í˜¸ì¶œ...")
        try:
            prompt = build_dependency_prompt(prev_config_scaled, prev_perf_scaled, curr_config_scaled, curr_perf_scaled)
            response = query_openai(prompt)
            logger.info(f"ğŸ’¬LLM Resposne: {response}")
            parsed_data = parse_llm_response(response)
            relation_type = parsed_data.get("relation_type"); knob_names_from_llm = parsed_data.get("knob_names")
            logger.info(f"ğŸ“ŠLLM ê²°ê³¼: ê´€ê³„='{relation_type}', Knobs='{knob_names_from_llm}'")
            # POSITIVE, INVERSE
            if relation_type in ["positive", "inverse"] and knob_names_from_llm and len(knob_names_from_llm) >= 2:
                knob1_name, knob2_name = knob_names_from_llm[0], knob_names_from_llm[1]

                A_prev_scaled, A_curr_scaled = prev_config_scaled[knob1_name], curr_config_scaled[knob1_name]
                B_prev_scaled, B_curr_scaled = prev_config_scaled[knob2_name], curr_config_scaled[knob2_name]

                if relation_type == "positive": weight = DependencyScore("positive", alpha=args.alpha).dependency_score_func_ver2(A_prev_scaled, A_curr_scaled, B_prev_scaled, B_curr_scaled)
                else: weight = DependencyScore("inverse", beta=args.beta).dependency_score_func_ver2(A_prev_scaled, A_curr_scaled, B_prev_scaled, B_curr_scaled)
            # THRESHOLD
            elif relation_type == "threshold":
                 threshold_knob_name, affected_knob_name = knob_names_from_llm[0], knob_names_from_llm[1]
                 threshold_value = parsed_data.get("threshold_value")

                 A_prev_scaled, A_curr_scaled = prev_config_scaled[threshold_knob_name], curr_config_scaled[threshold_knob_name]
                 B_prev_scaled, B_curr_scaled = prev_config_scaled[affected_knob_name], curr_config_scaled[affected_knob_name]
                 weight = DependencyScore("threshold", gamma=args.gamma).dependency_score_func_ver2(A_prev_scaled, A_curr_scaled, B_prev_scaled, B_curr_scaled, threshold_value)

            # NOTHING
            elif relation_type == "nothing": print("â›” INFO: LLM ë¶„ì„ ê²°ê³¼: ì˜ì¡´ì„± ì—†ìŒ."); weight = INITIAL_DEPENDENCY_WEIGHT
            else: logger.info("WARN: LLM ì‘ë‹µì—ì„œ ìœ íš¨ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨."); weight = INITIAL_DEPENDENCY_WEIGHT
        except KeyError as e: logger.info(f"âŒ ì˜¤ë¥˜: LLM ë°˜í™˜ knob ì´ë¦„ '{e}' ì—†ìŒ."); weight = INITIAL_DEPENDENCY_WEIGHT
        except Exception as e: logger.info(f"âŒ ì˜¤ë¥˜: LLM/ì˜ì¡´ì„± ê³„ì‚° ì¤‘ ë¬¸ì œ - {e}"); weight = INITIAL_DEPENDENCY_WEIGHT
    else: logger.info("ğŸ’€ INFO: ì„±ëŠ¥ ë³€í™” ë¯¸ë¯¸ (ìŠ¤ì¼€ì¼ë§ ê°’ ê¸°ì¤€), LLM í˜¸ì¶œ ê±´ë„ˆëœ€."); weight = INITIAL_DEPENDENCY_WEIGHT
    weight = max(0.1, float(weight))
    logger.info(f"INFO: ë‹¤ìŒ ë°˜ë³µ ê°€ì¤‘ì¹˜: {weight:.4f}")
    return weight



# --- ì»¤ìŠ¤í…€ Acquisition í•¨ìˆ˜ ---
class DependencyWeightedAcquisitionFunction(AcquisitionFunction):
    """ì»¤ìŠ¤í…€ Acquisition í•¨ìˆ˜"""
    # ì´ì „ ë‹µë³€ í´ë˜ìŠ¤ ë‚´ìš©ê³¼ ë™ì¼
    def __init__(self, model: Model, base_acquisition_function: AcquisitionFunction, dependency_weight: float):
        super().__init__(model=model); self.base_acqf = base_acquisition_function; self.dependency_weight = dependency_weight
    def forward(self, X: torch.Tensor) -> torch.Tensor:
        base_acqf_value = self.base_acqf(X); weight_tensor = torch.tensor(self.dependency_weight, device=X.device, dtype=X.dtype)
        return base_acqf_value * weight_tensor

# --- GP ëª¨ë¸ í•™ìŠµ í•¨ìˆ˜ ---
def get_fitted_model(train_X: torch.Tensor, train_Y: torch.Tensor) -> SingleTaskGP:
    """GP ëª¨ë¸ í•™ìŠµ (Y ë‚´ë¶€ í‘œì¤€í™” ì œê±°ë¨, [0, 1] ìŠ¤ì¼€ì¼ ì…ë ¥ ì‚¬ìš©)."""
    # ì´ì „ ë‹µë³€ í•¨ìˆ˜ ë‚´ìš©ê³¼ ë™ì¼ (ë‚´ë¶€ í‘œì¤€í™” ì œê±° ë²„ì „)
    logger.info("INFO: GP ëª¨ë¸ í•™ìŠµ ì‹œì‘..."); model = SingleTaskGP(train_X, train_Y); mll = ExactMarginalLogLikelihood(model.likelihood, model)
    try: fit_gpytorch_mll(mll); logger.info("INFO: GP ëª¨ë¸ í•™ìŠµ ì™„ë£Œ."); return model
    except Exception as e: logger.info(f"âŒ ì˜¤ë¥˜: GP ëª¨ë¸ í•™ìŠµ ì¤‘ ë¬¸ì œ - {e}"); raise e

# --- ì´ˆê¸° ìµœê³ ì  íƒìƒ‰ í•¨ìˆ˜ (ìŠ¤ì¼€ì¼ë§ëœ ê°’ ê¸°ì¤€) ---
def find_best_initial_point_scaled(scaled_score_Y_all_np: np.ndarray) -> Tuple[float, int]:
    """ì´ˆê¸° ë°ì´í„°ì…‹ì—ì„œ ìµœê³  ì„±ëŠ¥ ì§€ì ì˜ ì¸ë±ìŠ¤ì™€ ìŠ¤ì¼€ì¼ë§ëœ Yê°’ ì°¾ê¸°"""
    target_y_scaled = scaled_score_Y_all_np
    # ë¹„êµìš© ê°’ (ìµœì†Œí™” ë¬¸ì œ ì‹œ ë¶€í˜¸ ë°˜ì „)
    y_for_comparison = target_y_scaled if IS_MAXIMIZATION else -target_y_scaled

    best_idx = y_for_comparison.argmax()
    best_y_scaled = target_y_scaled[best_idx] # ì‹¤ì œ ìŠ¤ì¼€ì¼ë§ëœ ëª©í‘œê°’

    logger.info(f"INFO: ì´ˆê¸° ë°ì´í„° ìµœê³  ì„±ëŠ¥ (TPS/Latency, ìŠ¤ì¼€ì¼ë§ ê°’): {best_y_scaled:.4f} at index {best_idx}")
    return best_y_scaled, best_idx # ìŠ¤ì¼€ì¼ë§ëœ ìµœê³  Yê°’ê³¼ í•´ë‹¹ ì¸ë±ìŠ¤ ë°˜í™˜

# --- ì´ˆê¸° ëœë¤ ì§€ì  ì„ íƒ í•¨ìˆ˜ (ìŠ¤ì¼€ì¼ë§ëœ ê°’ ê¸°ì¤€) ---
def select_random_initial_point_scaled(scaled_score_Y_all_np: np.ndarray) -> Tuple[float, int]:
    """ì´ˆê¸° ë°ì´í„°ì…‹ì—ì„œ ëœë¤ ì§€ì ì˜ ì¸ë±ìŠ¤ì™€ ìŠ¤ì¼€ì¼ë§ëœ ëª©í‘œ Yê°’ì„ ì„ íƒí•©ë‹ˆë‹¤."""
    num_initial_points = scaled_score_Y_all_np.shape[0]
    if num_initial_points == 0:
        raise ValueError("ì´ˆê¸° ë°ì´í„°ì…‹ì´ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

    # 0ë¶€í„° (ë°ì´í„° ê°œìˆ˜ - 1) ì‚¬ì´ì˜ ì •ìˆ˜ ì¸ë±ìŠ¤ë¥¼ ëœë¤í•˜ê²Œ ì„ íƒ (ì´ë¯¸ scale ëœ ê°’.)
    random_idx = np.random.randint(0, num_initial_points)

    # ì„ íƒëœ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ìŠ¤ì¼€ì¼ë§ëœ ëª©í‘œ Y ê°’
    selected_y_score_scaled = scaled_score_Y_all_np[random_idx]
    # print(f"selected_y_score_scaled: {selected_y_score_scaled}")
    # print(f"selected_y_score_scaled.shape: {selected_y_score_scaled.shape}")

    logger.info(f"INFO: ì´ˆê¸° ì§€ì ìœ¼ë¡œ ëœë¤ ì„ íƒ (TPS/Latency, ìŠ¤ì¼€ì¼ë§ ê°’): {selected_y_score_scaled} at index {random_idx}")
    return selected_y_score_scaled, random_idx # ì„ íƒëœ ìŠ¤ì¼€ì¼ë§ëœ Yê°’ê³¼ í•´ë‹¹ ì¸ë±ìŠ¤ ë°˜í™˜


'''
[part 2] êµ¬í˜„
* í™œì„±â†’ë¹„í™œì„± ì „í™˜ ê¸°ë¡í•˜ëŠ” ìë£Œêµ¬ì¡°
* êµì²´ ì£¼ê¸°(Selection ì£¼ê¸°)ë§ˆë‹¤ LLMì„ í˜¸ì¶œí•´ â€œí˜„ì¬ ë¹„í™œì„± ì„¸íŠ¸ ì¤‘, í™œì„± ì„¸íŠ¸ì™€ ìƒê´€ëœ í›„ë³´â€ ì¶”ë¡ 
* ë­í‚¹ì— ë¶€ìŠ¤íŒ…í•´ì„œ ë³‘í•©
 

'''




# =============================================================================
# ë©”ì¸ ì‹¤í–‰ ë¡œì§
# =============================================================================

if __name__ == "__main__":

    BASE_DIR = "../../../data"
    CSV_FILE_PATH = os.path.join(BASE_DIR, f"workloads/mysql/original_data/preprocess/knob_filter/{args.workload}.csv") # MYSQL_YCSB_{AA/BB/EE/FF}_ORIGIN -> Historical Tuning Data
    MODEL_SAVE_DIR = os.path.join(BASE_DIR, "models/xgboost_mysql/scale") # pre-trained X, Y Scaler
    BASE_FILENAME = args.workload

    MODEL_PATH = os.path.join(MODEL_SAVE_DIR, f"{BASE_FILENAME}._model.pkl") # performance prediction model
    X_SCALER_PATH = os.path.join(MODEL_SAVE_DIR, f"{BASE_FILENAME}._x_scaler.pkl")
    Y_SCALER_PATH = os.path.join(MODEL_SAVE_DIR, f"{BASE_FILENAME}._y_scaler.pkl")
    KNOB_NAMES_PATH = os.path.join(MODEL_SAVE_DIR, f"{BASE_FILENAME}._knobs.pkl")
    # 1. ë°ì´í„° ë° êµ¬ì„± ìš”ì†Œ ë¡œë“œ
    df, xgb_model, x_scaler, y_scaler, knob_names = load_data_and_components(
        CSV_FILE_PATH, MODEL_PATH, X_SCALER_PATH, Y_SCALER_PATH, KNOB_NAMES_PATH
    )
    DIM = len(knob_names) # input dimension
    bounds = torch.tensor([[0.0] * DIM, [1.0] * DIM], device=DEVICE, dtype=DTYPE)

    # 2. ì´ˆê¸° ë°ì´í„° ì¤€ë¹„ ([0, 1] ìŠ¤ì¼€ì¼ í…ì„œ ë° ì›ë³¸ ë°°ì—´), train_Y = scaled_score(TPS/Latency)
    train_X, train_Y, X_all_orig, Y_all_orig = prepare_initial_data(
        df, knob_names, x_scaler, y_scaler
    )
    scaled_Y_all_np = y_scaler.transform(Y_all_orig) # ì—­ë³€í™˜ ìœ„í•´ ì›ë³¸ Yë„ ìŠ¤ì¼€ì¼ë§ -> ì´ê²Œ ë¨¼ì†”? ë¬´ìŠ¨ ì—­ë³€í™˜?
    # print(f"scaled_Y_all_np: {scaled_Y_all_np.shape}")
    epsilon = 1e-9
    # ì„±ëŠ¥ ì§€í‘œ = TPS/Latency
    scaled_score_Y_all_np = (
        scaled_Y_all_np[:, 0] / scaled_Y_all_np[:, 1] + epsilon #(1000,)
    )
    # print(f"scaled_score_Y_all_np: {scaled_score_Y_all_np.shape}")


    # 3. ì´ˆê¸° ìƒíƒœ ë° ìµœê³  ì„±ëŠ¥ ì¶”ì  ë³€ìˆ˜ ì´ˆê¸°í™”
    ## ìµœì´ˆ active set (ì´ˆê¸°ì—” TOP_Kë§Œ ì—´ì–´ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” best ê°’ìœ¼ë¡œ ê³ ì •)
    ## ì•„ì§ selectionì„ ëŒë¦¬ì§€ ì•Šì•˜ìœ¼ë‹ˆ ì¼ë‹¨ ì•ìª½ TOP_Kë¥¼ ì„ì‹œ í™œì„±í™”(ì•ˆì „í•œ ì´ˆê¸°í™”)
    # active_knob_indices = set(range(min(TOP_K, DIM)))
    # fixed_features_current = _build_fixed_features_for_inactive(DIM, active_knob_indices, best_x_scaled_tensor)
    # (1) ìµœì´ˆì—” activeë§Œ ì¡ê³ , fixed_featuresëŠ” ë‚˜ì¤‘ì— ë§Œë“ ë‹¤
    # active_knob_indices = set()  # ì„ì‹œ # (ì°¸ì¡°)

    # ... train_X, train_Y ì¤€ë¹„ ...
    prev_config_scaled = None
    prev_perf_scaled = None
    current_dependency_weight: float = INITIAL_DEPENDENCY_WEIGHT

    # (ì—¬ê¸°ì„œ ëœë¤ ì‹œì‘ì  ì„ ì • â€” í•¨ìˆ˜ ì •ì˜ê°€ ì´ë¯¸ ìœ„ì—ì„œ ëë‚œ ë’¤ì´ë¯€ë¡œ OK)
    # best_y_scaled, best_idx_init = select_random_initial_point_scaled(scaled_score_Y_all_np)
    # best_x_scaled_tensor = train_X[best_idx_init].unsqueeze(0)
    # (ì°¸ì¡°) random -> document defaultê°’
    defaults_scaled = load_mysql_defaults_scaled(knob_names, x_scaler) # (1, DIM)
    best_x_scaled_tensor = torch.tensor(defaults_scaled, device=DEVICE, dtype=DTYPE) # (1, DIM)
    
    # (ì°¸ì¡°) ë¬¸ì„œ ê¸°ë³¸ê°’ì—ì„œì˜ ëª©í‘œê°’(ìŠ¤ì¼€ì¼) ê³„ì‚° (defaultê°’ ì •ê·œí™” í•œ config -> ì„±ëŠ¥ ì˜ˆì¸¡ê°’ ë°˜í™˜)
    pred0 = predict_performance_scaled(best_x_scaled_tensor.cpu().numpy(), xgb_model)
    epsilon = 1e-9
    best_y_scaled = (tps_importance * pred0['scaled_tps']) / (pred0['scaled_latency'] + epsilon)
    
    # ì´ì œ DIM í™•ì • í›„ active/fixed ì´ˆê¸°í™”
    # active_knob_indices = set(range(min(TOP_K, DIM))) # (ì°¸ì¡°) -> ì´ê±¸ ì¼ë‹¨, historical workloadì—ì„œ selection í•œ ê²°ê³¼ë¡œ ê³ ì³ì•¼ í•˜ëŠ” ê±° ì•„ë‹Œê°€?
    HISTORY_TUNING_CSV = os.path.join(
        BASE_DIR, "workloads/mysql/original_data/preprocess", f"{args.workload}.csv"   
    )

    script_dir = os.path.dirname(os.path.abspath(__file__))
    out_dir = os.path.join(script_dir, "../../../data/bo_result/knob_selection_result")
    os.makedirs(out_dir, exist_ok=True)
    out_csv_path = os.path.join(
        out_dir, f"init_knob_selection_{args.workload}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    )

    ki_csv = KNOB_INFO_CSV if os.path.exists(KNOB_INFO_CSV) else None
    knob_selection_with_lasso(
        csv=HISTORY_TUNING_CSV,
        target_col=TARGET_FOR_SELECTION,      # ìš”ì²­í•œ ëŒ€ë¡œ TPS/Latencyë¡œ ì…€ë ‰ì…˜
        out_csv=out_csv_path,
        top_k=TOP_K,
        perf_cols=None,                # ë‚´ë¶€ ê¸°ë³¸ê°’ ì‚¬ìš©
        seed=RANDOM_STATE,
        knob_info_csv=ki_csv,          # ìˆìœ¼ë©´ ìŠ¤ì¼€ì¼ ì•ˆì •ì 
    )

    # 2) ì„ íƒëœ knob ì´ë¦„ ì½ê¸° -> ë‚´ ëª¨ë¸ì˜ knob_names ìˆœì„œì— ë§ëŠ” ì¸ë±ìŠ¤ë¡œ ë³€í™˜
    selected_names = _safe_read_selected_knobs_from_csv(out_csv_path)
    ranked_idx = _indices_of(selected_names, knob_names)
    print(f"ğŸ§  ranked_idx: {ranked_idx}") # (ì •ìƒ)
    print(f"ğŸ§  selected_names: {selected_names}") # (ì •ìƒ)

    # TOP_Kë¡œ ì œí•œ & ì§‘í•©í™”
    active_knob_indices = set(ranked_idx[:TOP_K])
    print(f"ğŸ§  active_knob_indices: {active_knob_indices}") # (ì •ìƒ)

    # ë¹„í™œì„± featureëŠ” í˜„ì¬ best ì„¤ì •(ìŠ¤ì¼€ì¼ ê³µê°„) ê°’ìœ¼ë¡œ ê³ ì •  
    fixed_features_current = _build_fixed_features_for_inactive(DIM, active_knob_indices, best_x_scaled_tensor)
    act_names = [knob_names[i] for i in sorted(active_knob_indices)]

    # ì½”ë“œ ì‚­ì œ (history_weights, history_best_y_scaled)

    # <<< ìˆ˜ì • ì‹œì‘: ìƒíƒœ ë³€ìˆ˜ë¥¼ ìŠ¤ì¼€ì¼ë§ëœ ê°’ìœ¼ë¡œ ì €ì¥ >>>
    prev_config_scaled: Optional[Dict[str, float]] = None
    prev_perf_scaled: Optional[Dict[str, float]] = None
    # <<< ìˆ˜ì • ë >>>
    current_dependency_weight: float = INITIAL_DEPENDENCY_WEIGHT # 1.0


    ## ((ì¶”ê°€)) ê²°ê³¼ ì €ì¥ì„ ìœ„í•œ ë¦¬ìŠ¤íŠ¸
    history_weights = []
    history_best_y_scaled = [best_y_scaled]

    # tuning data ì €ì¥í•˜ëŠ” ì½”ë“œ 
    HISTORICAL_TUNING_DATA_FILE_PATH = os.path.join(BASE_DIR, f"workloads/mysql/original_data/preprocess/historical_data_{args.workload}.csv")
    ORIGINAL_TUNING_DATA_FILE_PATH = os.path.join(BASE_DIR, f"workloads/mysql/original_data/preprocess/MYSQL_YCSB_AA_ORIGIN.csv")
    
    # 4. ë² ì´ì¦ˆ ìµœì í™” ë£¨í”„
    logger.info(f"\n=== ë² ì´ì¦ˆ ìµœì í™” ì‹œì‘ ({N_ITERATIONS}íšŒ ë°˜ë³µ) ===")
    for iteration in range(1, N_ITERATIONS + 1):
        if iteration % SELECTION_PERIOD == 0:
            try:
            # [Dynamic Knob Selection ë¡œì§ ì‘ì„±]
            # ** botorch - fixed_feature ì‚¬ìš©í•´ì„œ input_dimension ëŠ˜ë ¤ì•¼ í•¨.(top-k selection ì‹œ, 2*k í¬ê¸°ë¡œ ê¸°ë³¸ ì„¸íŒ…)    
            # 1) temp_dataì— ë„£ì–´ë†¨ë‹¤ê°€, ì´ ì¡°ê±´ë¬¸ ì•ˆì— ë“¤ì–´ì˜¤ë©´, ì „ì²´ json íŒŒì¼ê³¼ í•©ì³ì£¼ê¸°
            # 2) ê¸°ì¡´ Lassoë¥¼ ì´ìš©í•œ knob selectionê³¼ ë§ˆì°¬ê°€ì§€ë¡œ Knob Selection Ranking ì¶”ì¶œ
            ## -> Ranking Score í•¨ê»˜ ì €ì¥í•˜ë„ë¡ í•¨. -> (i-1)ë²ˆì§¸ Ranking Score ìˆœì„œ + (i)ë²ˆì§¸ Ranking Score ìˆœì„œëŒ€ë¡œ ì •ë ¬ 
            ##      -> ê²¹ì¹˜ëŠ” Knob ìˆì„ ì‹œ countí•´ì„œ ë‹¤ìŒ Raking Knobì„ ì¶”ê°€
            ##      -> Knob Selection ê²°ê³¼ prev_knob_selection.txtì— ì €ì¥
            # 3) 2)ì˜ ê²°ê³¼ë¡œ ë‚˜ì˜¨ 'fixed_knob_set' + 'current_important_knob_set.txt'ë¥¼ LLMì— ì£¼ê³  knob correlation ì¶”ë¡  ì§„í–‰
            ## -> if) 'fixed_knob_set'ì— correlationì„ ê°–ëŠ” knobì´ ì¡´ì¬
            ##         then) botorch fixed_feature ì¼ë¶€ í•´ì œí•˜ì—¬ ë™ì ìœ¼ë¡œ ë³€í•˜ëŠ” input dimension í™•ì¥í•˜ê¸°
            ## -> else) pass

            # 3) Knob Subset êµì²´
            ## -> ì „ì²´ knob set í¬ê¸°ì˜ ëª‡ % ê¹Œì§€ êµì²´ ê°€ëŠ¥í•˜ê²Œ í•  ì§€ ì„¤ì •(default: ì „ì²´ knob setì˜ 20%)
            ## -> êµì²´í•  ì‹œ, ìµœëŒ€í•œ featureì˜ ì˜ë¯¸ê°€ ë³€í•˜ì§€ ì•Šë„ë¡, ê°™ì€ knobì„ ì˜ë¯¸í•˜ëŠ” ê²½ìš° ìë¦¬ë¥¼ êµì²´í•˜ì§€ X (ì´ê±° ì–´ë–¤ì‹ìœ¼ë¡œ ìˆ˜í–‰í•  ì§€ ì•Œê³ ë¦¬ì¦˜ í™•ì‹¤í•˜ê²Œ ì •ë¦½í•´ì•¼ í•  ë“¯.)
            ## -> Knob Subset êµì²´ í›„, bo search ì´ì „ê³¼ ê°™ì´ ì§„í–‰.
                print("=== ğŸ” Dynamic Knob Selection triggered ===")
                selection_input_csv = ORIGINAL_TUNING_DATA_FILE_PATH # ì‹¤ì‹œê°„ìœ¼ë¡œ data append
                if not os.path.exists(selection_input_csv):
                    # íˆìŠ¤í† ë¦¬ê°€ ì•„ì§ ì—†ë‹¤ë©´, ê¸°ì¡´ ë°ì´í„°ì…‹(df)ë¡œë¼ë„ ê°€ë³ê²Œ selection ì‹œë„
                    # dfì—ëŠ” knob+perfê°€ ë‹´ê²¨ ìˆìœ¼ë¯€ë¡œ ì„ì‹œ íŒŒì¼ë¡œ ì €ì¥
                    with tempfile.NamedTemporaryFile(mode="w", suffix=".csv", delete=False) as tmp:
                        df.to_csv(tmp.name, index=False)
                        selection_input_csv = tmp.name
                        print("WARN: history csv not found; used initial df snapshot for selection.")
                
                # out csv ê²½ë¡œ(ì„ì‹œ)
                # (todo) ê²°ê³¼íŒŒì¼ ì €ì¥ë˜ëŠ” ê²½ë¡œ ì •í™•íˆ ì–´ë”˜ì§€?
                script_dir = os.path.dirname(os.path.abspath(__file__))
                out_dir = os.path.join(script_dir, "../../../data/bo_result/knob_selection_result") # ./outputs
                os.makedirs(out_dir, exist_ok=True)
                out_csv_path = os.path.join(out_dir, f"knob_selection_result_{args.workload}_{iteration}.csv")

                # 2) Lasso ê¸°ë°˜ selection ì‹¤í–‰ (knob_info ìˆìœ¼ë©´ ìŠ¤ì¼€ì¼ ì•ˆì •)
                ki_csv = KNOB_INFO_CSV if os.path.exists(KNOB_INFO_CSV) else None

                knob_selection_with_lasso(
                    csv=selection_input_csv,
                    target_col=TARGET_FOR_SELECTION,
                    out_csv=out_csv_path,
                    top_k=TOP_K,
                    perf_cols=None,   # ë‚´ë¶€ ê¸°ë³¸ ì‚¬ìš©
                    seed=RANDOM_STATE,
                    knob_info_csv=ki_csv,
                )
                selected_names = _safe_read_selected_knobs_from_csv(out_csv_path)
                # log
                logger.info("ğŸ  selected_names[:10] = %s len=%d", selected_names[:10], len(selected_names))
                logger.info("ğŸ  knob_names[:10]     = %s", knob_names[:10])
                # (ì‚­ì œ) logger.info("ğŸ  indices[:10]        = %s", _indices_of(selected_names, knob_names)[:10])
                ranked_new_idx = _indices_of(selected_names, knob_names)  # ì´ë¦„â†’index
                

                if not ranked_new_idx:
                    logger.info("WARN: selection produced empty set; keeping previous active subset.")
                else:
                    # 3) ì œí•œëœ êµì²´ ì •ì±…ìœ¼ë¡œ í™œì„± ì§‘í•© ê°±ì‹ 
                    new_active = _merge_active_set_with_limit(
                        prev_active=active_knob_indices,
                        ranked_new=ranked_new_idx,
                        dim=DIM,
                        replacement_ratio=REPLACEMENT_RATIO,
                        top_k=TOP_K,
                    )

                    # (Part 2) êµ¬í˜„: "í™œì„± -> ë¹„í™œì„± ì „í™˜" ì´ë ¥ ê¸°ë¡
                    prev_active_sorted = sorted(list(active_knob_indices))
                    new_active_sorted = sorted(list(new_active))
                    deactivated = [i for i in prev_active_sorted if i not in new_active_sorted]
                    now_ts= datetime.now()
                    for i in deactivated:
                        deactivated_history.append((i, knob_names[i], now_ts))
                    if deactivated:
                        logger.info("ğŸ§¾ Deactivated this round: %s", [knob_names[i] for i in deactivated])
                    
                    # (Part 2) êµ¬í˜„: LLM ê¸°ë°˜ 'ì¶”ê°€ í™œì„±(í™•ì¥)' ë‹¨ê³„
                    try:
                        active_names_now = [knob_names[i] for i in sorted(list(new_active))]
                        inactive_cum_names = _cumulative_inactive_names(limit=1000)

                        llm_names= _llm_select_correlated_from_sets(
                            active_names = active_names_now,
                            inactive_names = inactive_cum_names,
                            logger = logger,
                            max_return = 10,
                        )
                        
                        # ì´ë¦„ + ì¸ë±ìŠ¤
                        name_to_idx = {n: i for i, n in enumerate(knob_names)}
                        llm_indices = [name_to_idx[n] for n in llm_names if n in name_to_idx]

                        # 'ì¶”ê°€ í™œì„±' í—ˆìš© -> ì§€ê¸ˆ inactiveì¸ ê²ƒë“¤ë§Œ ì„ íƒ
                        currently_inactive = [i for i in llm_indices if i not in new_active]

                        extra_add = currently_inactive[:EXTRA_ACTIVATE_MAX]

                        if extra_add:
                            expanded_active = set(new_active) | set(extra_add)
                            added_names = [knob_names[i] for i in extra_add]
                            logger.info("â• Expand active (+%d): %s", len(extra_add), added_names)
                            new_active = expanded_active
                        else:
                            logger.info("â„¹ï¸ No extra activation from LLM this round.")
                    except Exception as e:
                        logger.info(f"WARN: LLM-based expansion skipped: {e}")

                    # 4) ë¹„í™œì„± featureëŠ” best ì„¤ì •ìœ¼ë¡œ ê³ ì • (ìŠ¤ì¼€ì¼ ê³µê°„)
                    new_fixed = _build_fixed_features_for_inactive(DIM, new_active, best_x_scaled_tensor)

                    # 5) ìƒíƒœ ê°±ì‹ 
                    active_knob_indices = new_active                                                                                                                                                                                                                                                     
                    fixed_features_current = new_fixed 

                    # 6) ë¤í”„ + ë¡œê·¸ 
                    _dump_selection(knob_names, active_knob_indices, SELECTION_DUMP_PATH)
                    act_names = [knob_names[i] for i in sorted(list(active_knob_indices))]
                    logger.info(f"âœ… Active subset ({len(active_knob_indices)}): {act_names}")
                    logger.info(f"âœ… Fixed features (inactive count={len(new_fixed)}).")
            except Exception as e:
                logger.info(f"âŒ Dynamic selection failed: {e}. Keep previous subset.")
             
            print("=== ğŸ” Dynamic Knob Selection ì¢…ë£Œ ğŸ” ===")

        logger.info(f"\n--- ë°˜ë³µ {iteration}/{N_ITERATIONS} ---")

        # --- GP ëª¨ë¸(surrogate model) í•™ìŠµ ---
        try:
            gp_model = get_fitted_model(train_X, train_Y)
        except Exception:
            logger.info("WARN: GP ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨, ì´ë²ˆ ë°˜ë³µ ê±´ë„ˆëœ€."); continue
            
        # history_best_y_scaled.append(best_y_scaled)

        # --- Acquisition Function ì¤€ë¹„ ---
        # ucb = UpperConfidenceBound(model=gp_model, beta=6.25) # exploration(betaê°€ ì»¤ì§ˆìˆ˜ë¡ íƒìƒ‰ ë²”ìœ„ ë„“ì–´ì§)
        base_ei = ExpectedImprovement(model=gp_model, best_f=best_y_scaled, maximize=True)
        # Acquisition Function: GP(surrogate model)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŒ
        custom_acqf = DependencyWeightedAcquisitionFunction(gp_model, base_ei, current_dependency_weight)
        logger.info(f"INFO: íšë“ í•¨ìˆ˜ í˜„ì¬ ì ìš© ê°€ì¤‘ì¹˜: {current_dependency_weight:.4f}")

        # --- ë‹¤ìŒ í›„ë³´ ì§€ì  íƒìƒ‰ ([0, 1] ìŠ¤ì¼€ì¼) ---
        logger.info("INFO: ë‹¤ìŒ í›„ë³´ ì§€ì  íƒìƒ‰ ì¤‘...")
        try:
            candidate_normalized, acqf_value = optimize_acqf(
                custom_acqf, bounds=bounds, q=1, num_restarts=10, raw_samples=1024,
                options={"batch_limit": 5, "maxiter": 200},
                fixed_features = fixed_features_current # (ì°¸ì¡°) dynamic knob selection -> input dimension í™•ì¥ì„ ìœ„í•œ ì»¤ìŠ¤í…€ í•¨ìˆ˜.(ë¹„í™œì„± knob ì„ì‹œ ê³ ì •)
            )
            logger.info("INFO: í›„ë³´ ì§€ì  íƒìƒ‰ ì™„ë£Œ.")
        except Exception as e:
            logger.info(f"âŒ ì˜¤ë¥˜: Acq Func ìµœì í™” ì¤‘ - {e}\nWARN: ì´ë²ˆ ë°˜ë³µ ê±´ë„ˆëœ€."); continue

        # --- í›„ë³´ ì§€ì  ì„±ëŠ¥ ì˜ˆì¸¡ (ìŠ¤ì¼€ì¼ë§ëœ ê°’ ì‚¬ìš©) ---
        scaled_candidate_np = candidate_normalized.cpu().numpy()
        curr_perf_dict_scaled: Optional[Dict[str, float]] = predict_performance_scaled(scaled_candidate_np, xgb_model)

        if curr_perf_dict_scaled is None:
            logger.info("WARN: í›„ë³´ ì§€ì  ì„±ëŠ¥ ì˜ˆì¸¡ ì‹¤íŒ¨, ì´ë²ˆ ë°˜ë³µ ê±´ë„ˆëœ€.")
            history_weights.append(current_dependency_weight)
            history_best_y_scaled.append(best_y_scaled)
            continue

        epsilon = 1e-9
        new_objective_value_scaled: float = (tps_importance*curr_perf_dict_scaled['scaled_tps']) / (curr_perf_dict_scaled['scaled_latency']+epsilon)
        logger.info(f"  - ì˜ˆì¸¡ëœ ì„±ëŠ¥ (TPS/Latency, ìŠ¤ì¼€ì¼ë§ ê°’): {new_objective_value_scaled:.4f}")

        ############################  save tuning history data   ############################       
        # (ì°¸ì¡°)
        info_df = pd.read_csv("Knob_Information_MySQL_v5.7.csv")
        info_df.columns = info_df.columns.str.strip().str.replace("\ufeff", "", regex=False)
        # ì‹¤ì œ csv ì»¬ëŸ¼ëª… ë§ì¶°ì£¼ê¸°(ex) Name, Default Value)
        default_values = dict(zip(info_df['name'], info_df['d_f_default']))
        # print(f"ğŸ¢ğŸ¢ğŸ¢default values: {default_values}")
        full_knob_list = list(default_values.keys())

        # # tuning data ì €ì¥í•˜ëŠ” ì½”ë“œ 
        # HISTORICAL_TUNING_DATA_FILE_PATH = os.path.join(BASE_DIR, f"workloads/mysql/original_data/preprocess/historical_data_{args.workload}.csv")
        # ORIGINAL_TUNING_DATA_FILE_PATH = os.path.join(BASE_DIR, f"workloads/mysql/original_data/preprocess/MYSQL_YCSB_AA_ORIGIN.csv")
        # iteration == 1ì´ë©´ csv header ì´ˆê¸°í™”
        # (ì°¸ì¡°) (ë°”ê¿”ì•¼ í•  ë¶€ë¶„) -> (ì´ë¯¸ ë°”ê¾¼ê±´ê°€??) -> real-time tuning data ìˆ˜ì§‘
        if os.path.exists(ORIGINAL_TUNING_DATA_FILE_PATH):
            hist_df = pd.read_csv(ORIGINAL_TUNING_DATA_FILE_PATH)
        else: # teration == 1 and not os.path.exists(HISTORICAL_TUNING_DATA_FILE_PATH):
            with open(HISTORICAL_TUNING_DATA_FILE_PATH, mode = 'w', newline = '') as f:
                writer = csv.writer(f)
                # header = knob_names + ['tps', 'latency']
                header = full_knob_list + ['tps', 'latency'] # (ì°¸ì¡°)
                writer.writerow(header)


        # 1) í›„ë³´ config ì—­ìŠ¤ì¼€ì¼ë§
        candidate_unscaled = x_scaler.inverse_transform(candidate_normalized.cpu().numpy())

        # 2) ì˜ˆì¸¡ëœ ì„±ëŠ¥ ì—­ìŠ¤ì¼€ì¼ë§
        perf_unscaled = y_scaler.inverse_transform(
            [[curr_perf_dict_scaled['scaled_tps'], curr_perf_dict_scaled['scaled_latency']]]
        )

        # tuningëœ knob ê°’ ë§¤í•‘ -> í˜„ì¬ íŠœë‹í•œ knobë“¤ì˜ ì‹¤ì œ ê°’
        tuned_knob_dict = dict(zip(knob_names, candidate_unscaled[0]))


        # 3) CSV í–‰ êµ¬ì„±
        # row = list(candidate_unscaled[0]) + list(perf_unscaled[0])
        row = []
        for knob in full_knob_list:
            value = tuned_knob_dict.get(knob, default_values[knob]) # tuning ì•ˆ ëœ knobì€ default ì‚¬ìš©
            row.append(value)

        row += list(perf_unscaled[0]) # [tps, latency]

        # 4) csv fileì— append
        # (ì°¸ì¡°) (ë°”ê¿”ì•¼ í•  ë¶€ë¶„)
        with open(ORIGINAL_TUNING_DATA_FILE_PATH, mode='a', newline = '') as f:
            writer = csv.writer(f)
            writer.writerow(row)
        #####################################################################################


        # ì˜ì¡´ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
        curr_config_scaled_dict = {knob_names[i]: scaled_candidate_np[0, i] for i in range(DIM)}
        logger.info("INFO: ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ ì˜ì¡´ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°")
        try:
            # (ì°¸ì¡°) ì˜ì¡´ì„± ê°€ì¤‘ì¹˜ ê³„ì‚°
            next_dependency_weight = calculate_dependency_weight(prev_config_scaled, prev_perf_scaled, curr_config_scaled_dict, curr_perf_dict_scaled, knob_names)
        except Exception as e:
            logger.info(f"âŒ {e}. ê¸°ë³¸ ê°€ì¤‘ì¹˜ ì‚¬ìš©.")
            next_dependency_weight = INITIAL_DEPENDENCY_WEIGHT


        # --- GP ë°ì´í„° ì—…ë°ì´íŠ¸(ìƒˆë¡œìš´ config ì¶”ê°€) ---
        new_objective_value_scaled_tensor = torch.tensor(
            [[new_objective_value_scaled]], device=DEVICE, dtype=DTYPE
        )
        train_X = torch.cat([train_X, candidate_normalized], dim=0)
        train_Y = torch.cat([train_Y, new_objective_value_scaled_tensor], dim=0)
        logger.info(f"INFO: GP í•™ìŠµ ë°ì´í„° ì—…ë°ì´íŠ¸ ì™„ë£Œ. í˜„ì¬ ë°ì´í„° ìˆ˜: {train_X.shape[0]}")

        # --- ìµœê³  ì„±ëŠ¥ ì—…ë°ì´íŠ¸ (ìŠ¤ì¼€ì¼ë§ëœ ê°’ ê¸°ì¤€) ---
        # print(f"best_y_scaled: {best_y_scaled}")
        objective_value_for_comparison = new_objective_value_scaled if IS_MAXIMIZATION else -new_objective_value_scaled
        best_y_scaled_comparison = best_y_scaled if IS_MAXIMIZATION else -best_y_scaled

        # logger.info(f"objective_value_for_comparison: {objective_value_for_comparison}")
        # logger.info(f"best_y_scaled_comparison: {best_y_scaled_comparison}")
        # print(f"objective_value_for_comparison: {objective_value_for_comparison}")
        # print(f"best_y_scaled_comparison: {best_y_scaled_comparison}")
        if objective_value_for_comparison > best_y_scaled_comparison:
            best_y_scaled = new_objective_value_scaled
            best_x_scaled_tensor = candidate_normalized # (ì°¸ì¡°) ìƒˆ í›„ë³´ì§€ì  ì„ íƒ
            logger.info(f" M_")
            logger.info(f"| âœ¨ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥ ë°œê²¬! (TPS/Latency, ìŠ¤ì¼€ì¼ë§ ê°’: {best_y_scaled:.4f}) âœ¨ |")
            logger.info(f" L_")
        else:
            logger.info(f"INFO: ìµœê³  ì„±ëŠ¥ ìœ ì§€ (TPS/Latency, ìŠ¤ì¼€ì¼ë§ ê°’: {best_y_scaled:.4f})")

        prev_config_scaled = curr_config_scaled_dict
        prev_perf_scaled = curr_perf_dict_scaled

        current_dependency_weight = next_dependency_weight

        history_weights.append(current_dependency_weight)
        history_best_y_scaled.append(best_y_scaled)


    # 5. ìµœì¢… ê²°ê³¼ ì¶œë ¥ (ì—­ìŠ¤ì¼€ì¼ë§)

    logger.info("\n=== ìµœì í™” ì™„ë£Œ ===")
    if best_x_scaled_tensor is not None:

        best_x_scaled_np = best_x_scaled_tensor.cpu().numpy()
        final_perf_scaled_dict = predict_performance_scaled(best_x_scaled_np, xgb_model) # (ì°¸ì¡°) ì„±ëŠ¥ ì˜ˆì¸¡
        # ... ì •ê·œí™” ë°ì´í„° ì—­ë³€í™˜ ...
        if final_perf_scaled_dict:
             final_tps_scaled = final_perf_scaled_dict['scaled_tps']; final_latency_scaled = final_perf_scaled_dict['scaled_latency']
             final_perf_unscaled = y_scaler.inverse_transform([[final_tps_scaled, final_latency_scaled]])
             final_tps_unscaled = final_perf_unscaled[0, 0]; final_latency_unscaled = final_perf_unscaled[0, 1]
        else:
             logger.info("WARN: ìµœì¢… ì„±ëŠ¥ ì˜ˆì¸¡ ì‹¤íŒ¨. ì €ì¥ëœ ëª©í‘œê°’ë§Œ ì—­ë³€í™˜ ì‹œë„.")
             temp_y_scaled = np.zeros((1, 2))
             final_perf_unscaled = y_scaler.inverse_transform(temp_y_scaled)
             final_tps_unscaled = np.nan
             final_latency_unscaled = np.nan
        final_x_unscaled_np = x_scaler.inverse_transform(best_x_scaled_np)
        final_x_unscaled_config = {knob_names[j]: final_x_unscaled_np[0, j] for j in range(DIM)}
        logger.info("ğŸ† ìµœì¢… ìµœì  ì„¤ì • (ì›ë˜ ìŠ¤ì¼€ì¼):")
        config_str = ", ".join([f"{k}: {v:.4f}" if isinstance(v, float) else f"{k}: {v}" for k, v in final_x_unscaled_config.items()])
        logger.info(f"  - Knobs: {{{config_str}}}")
        logger.info(f"  - ì„±ëŠ¥ (TPS): {final_tps_unscaled:.4f}")
        logger.info(f"  - ì„±ëŠ¥ (Latency): {final_latency_unscaled:.4f}")
        final_target_metric_value = final_tps_unscaled/final_latency_unscaled
    else:
        logger.info("âŒ ìµœì  ì„¤ì •ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
